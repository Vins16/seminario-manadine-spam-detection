{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado\n",
    "\n",
    "En este notebook utilizaremos los conjuntos que hemos inspeccionado, adecuado y preprocesado para entrenar modelos que nos ayuden a analizar los mensajes de texto y detectar si se trata de **spam** o no.\n",
    "\n",
    "El siguiente script está dividido en los siguientes bloques:\n",
    "\n",
    "- **BLOQUE A**: carga de datos preprocesados.\n",
    "- **BLOQUE B**: entrenamiento y inferencia con distintos modelos de ML.\n",
    "- **BLOQUE C**: entrenamiento y inferencia con una red neuronal (librería Keras).\n",
    "- **BLOQUE D**: transfer learning con un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE A: Carga de datos\n",
    "Antes de comenzar, cargaremos los datos que han sido adecuados en nuestra fase anterior de preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga textos vectorizados\n",
    "with open('../data/x_train_vec.pkl', 'rb') as f:\n",
    "    X_train_vec = pickle.load(f)\n",
    "\n",
    "with open('../data/x_test_vec.pkl', 'rb') as f:\n",
    "    X_test_vec = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga conjuntos de las etiquetas\n",
    "with open('../data/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open('../data/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE B: Entrenamiento de distinto modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logistica\n",
    "\n",
    "La regresión logística es un modelo estadístico para estudiar las relaciones entre un conjunto de variables cualitativas Xi y una variable cualitativa Y. Utilizando la función sigmoide, asigna valores entre 0 y 1, facilitando la predicción de categorías, como positivo o negativos, spam o no spam, en aplicaciones prácticas.\n",
    "\n",
    "Más información acerca de este modelo [aquí](https://cienciadedatos.net/documentos/py17-regresion-logistica-python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "log_model = ???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento o ajuste del modelo con los datos de entrenamiento\n",
    "log_model.???(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de entrenamiento\n",
    "y_pred_train = log_model.???(X_train_vec)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(classification_report(???, ???))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de test\n",
    "y_pred_test = log_model.???(X_test_vec)\n",
    "\n",
    "# Mostramos el \"classification report\" y \"accuracy\"\n",
    "accuracy = accuracy_score(???, ???)\n",
    "print('Resultados conjunto de test:\\n')\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "print(classification_report(???, ???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es Boosting?\n",
    "\n",
    "Boosting es un meta-algoritmo de aprendizaje automático que reduce el sesgo y la varianza en un contexto de aprendizaje supervisado. Consiste en combinar los resultados de varios clasificadores débiles para obtener un clasificador robusto. Cuando se añaden estos clasificadores débiles, se hace de modo que éstos tengan diferente peso en función de la exactitud de sus predicciones. Tras añadir un clasificador débil, los datos cambian su estructura de pesos: los casos mal clasificados ganan peso y los que son clasificados correctamente pierden peso.\n",
    "\n",
    "**Gradient Boosting (GB)** o Potenciación del gradiente consiste en plantear el problema como una optimización numérica en el que el objetivo es minimizar una función de coste añadiendo clasificadores débiles mediante el descenso del gradiente. Involucra tres elementos:\n",
    "\n",
    "- La **función de coste** a optimizar: depende del tipo de problema a resolver.\n",
    "- Un **clasificador débil** para hacer las predicciones: por lo general se usan árboles de decisión.\n",
    "- Un **modelo que añade (ensambla) los clasificadores débiles para minimizar la función de coste**: se usa el descenso del gradiente para minimizar el coste al añadir árboles.\n",
    "\n",
    "Los hiperparámetros más importantes que intervienen en este algoritmo (aunque no todos) son:\n",
    "\n",
    "- **learning_rate**: determina el impacto de cada árbol en la salida final. Se parte de una estimación inicial que se va actualizando con la salida de cada árbol. Es el parámetro que controla la magnitud de las actualizaciones.\n",
    "- **n_estimators**: número de clasificadores débiles a utilizar.\n",
    "\n",
    "Como en este caso utilizaremos árboles de decisión como clasificadores débiles a ensamblar, también debemos tener en cuenta los hiperparámetros que afectan a esta clase de modelos. En este caso:\n",
    "\n",
    "- **max_depth**: profundidad máxima del árbol.\n",
    "\n",
    "Más información sobre el modelo que se utiliza en este ejemplo y de sus parámetros [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo introduciendo los valores de los parámetros:\n",
    "gb_clf = ???(n_estimators=150, learning_rate=0.2, max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento o ajuste del modelo con los datos de entrenamiento\n",
    "gb_clf.???(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de entrenamiento\n",
    "pred_train = gb_clf.???(X_train_vec)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(???(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de test\n",
    "pred_test = gb_clf.???(X_test_vec)\n",
    "\n",
    "# Mostramos el \"classification report\" y \"accuracy\"\n",
    "accuracy = accuracy_score(???, ???)\n",
    "print('Resultados conjunto de test:\\n')\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "print(???(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE C: entrenamiento y inferencia con una red neuronal (librería Keras)\n",
    "\n",
    "Las redes neuronales artificiales son una rama de la Inteligencia Artificial que se basa en la simulación de la estructura y funcionamiento del cerebro humano para procesar información.\n",
    "\n",
    "Consisten en una serie de nodos interconectados que reciben información, la procesan y producen una salida. Dichos nodos, conocidos como neuronas artificiales, pueden ser ajustados para optimizar la salida de la red, lo que permite que la red aprenda y se adapte a diferentes tipos de entradas.\n",
    "\n",
    "*Curiosidad: cuando una red neuronal tiene simplemente más de tres capas, nace el conocido como **Deep Learning** o aprendizaje profundo.*\n",
    "\n",
    "Más información sobre este tipo de modelos [aquí](https://openwebinars.net/blog/que-son-las-redes-neuronales-y-sus-aplicaciones/).\n",
    "\n",
    "En este caso, entrenaremos la red para que también obtenga una representación de los textos analizados de manera directa, sin necesidad de utilizar el modelo de vectorizado TF-IDF antes calculado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset (ajusta la ruta según el archivo que tengas)\n",
    "df = pd.read_csv(???)\n",
    "\n",
    "# Verifica el dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento del texto\n",
    "X = df[???]  # Supón que la columna 'text' contiene los mensajes\n",
    "y = df[???]  # 'label' debe ser spam/no spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las etiquetas 'spam'/'ham' a números\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.???(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Que valores asume la variale target transformada? \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿A qué corresponden los nuevos valores?\n",
    "df[???].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = ???(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización y padding para convertir el texto en secuencias de enteros\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.???(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.???(X_train)\n",
    "X_test_seq = tokenizer.???(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Como son transformados los textos con Tokenizer?\n",
    "X_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding para asegurar que todas las secuencias tengan la misma longitud\n",
    "X_train_pad = ???(X_train_seq, padding='post', maxlen=100)\n",
    "X_test_pad = ???(X_test_seq, padding='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructura de la red neuronal:\n",
    "1. Capa de Embedding: \n",
    "    - input_dim=10000: Esto se refiere al tamaño del vocabulario (número total de palabras). \n",
    "\n",
    "    - output_dim=64: Este especifica la dimensión de los embeddings. En lugar de representar cada palabra con un valor binario o en un vector disperso (como ocurre con TF-IDF), las palabras se representarán por un vector denso de 64 dimensiones. \n",
    "\n",
    "    - input_length=100: se define la longitud máxima de las secuencias de entrada. Cada entrada de texto se convierte en una secuencia de índices de palabras (tokens), y si la longitud de una secuencia es menor que 100, se rellenará con ceros. Si es mayor, se truncará.\n",
    "2. Capa de Pooling: Esta capa realiza un *pooling global promedio* sobre las secuencias de embeddings. Dado que las entradas son secuencias de longitud 100, esta capa tomará el promedio de los vectores de embeddings de las 100 palabras (tokens) en la secuencia, lo que reduce la representación de la secuencia de una matriz de 100x64 a un solo vector de 64 dimensiones\n",
    "\n",
    "3. Capa densa (Fully connected): Esta capa toma la salida del pooling (un vector de 64 dimensiones) y lo pasa a través de una **capa densa con 64 neuronas** y la **función de activación ReLU**. Esto ayuda a la red a aprender características más complejas del texto procesado.\n",
    "\n",
    "4. Capa de salida: esta capa tiene una sola neurona con la **función de activación Sigmoid**, que es adecuada para clasificación binaria (en este caso, para clasificar como spam o no spam). La salida de esta capa será un valor entre 0 y 1, que puedes interpretar como la probabilidad de que un mensaje sea spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo simple de red neuronal para clasificación de texto\n",
    "nn_simple = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=???, output_dim=???, input_length=???),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(???, activation=???),\n",
    "    tf.keras.layers.Dense(???, activation=???)  # Para clasificación binaria (spam/no spam)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "nn_simple.???(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "nn_simple.???(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de test\n",
    "pred_test = nn_simple.???(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué contiene pred_test? ¿Qué son?\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las predicciones a clases (spam o no spam)\n",
    "# La salida será un valor entre 0 y 1 (probabilidad de spam)\n",
    "predicted_labels = (pred_test > ???).astype(int)  # Clasifica como spam (1) si la probabilidad > 0.5, sino no spam (0)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el \"classification report\" y \"accuracy\"\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print('Resultados conjunto de test:\\n')\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "print(classification_report(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning con un modelo preentrenado\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) es una técnica basada en redes neuronales para el pre-entrenamiento del procesamiento del lenguaje natural (NLP) desarrollada por Google. Se trata de un codificador que obtiene representaciones bidireccionales, lo que significa que puede entender el significado de una palabra en relación con las palabras que la rodean. Esto permite al modelo aprender a **interpretar el lenguaje en general y ser utilizado como modelo base**, de manera que, en caso de querer especializarlo en una tarea en particular (detección de spams, por ejemplo), solo es necesario añadir unas capas adicionales al mismo.\n",
    "\n",
    "El modelo original de BERT se entrenó utilizando dos grandes conjuntos de datos en lengua inglesa: BookCorpus y Wikipedia en inglés. \n",
    "\n",
    "*Curiosidad: actualmente, Google utiliza BERT en su motor de búsqueda para perfeccionar la comprensión de las búsquedas de los usuarios.*\n",
    "\n",
    "Por su parte, **DistilBERT** (Distilled version of BERT) es una versión más ligera, rápida y menos costosa del modelo BERT, como su propio nombre indica.\n",
    "\n",
    "Más información acerca de BERT [aquí](https://huggingface.co/blog/bert-101), y de DistilBERT [aquí](https://huggingface.co/docs/transformers/model_doc/distilbert) y [aquí](https://medium.com/huggingface/distilbert-8cf3380435b5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizer de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización de los textos\n",
    "X_train_enc = tokenizer(list(X_train[:100]), padding=True, truncation=True, max_length=100, return_tensors='tf') # Usamos 100 observaciones para poder ejecutarlo en local\n",
    "X_test_enc = tokenizer(list(X_test), padding=True, truncation=True, max_length=100, return_tensors='tf')\n",
    "\n",
    "# Cargar el modelo preentrenado DistilBERT para clasificación de texto\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compilar el modelo\n",
    "model_bert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento con transfer learning\n",
    "model_bert.fit(X_train_enc['input_ids'], y_train[:100], epochs=3, batch_size=128, validation_data=(X_test_enc['input_ids'], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos sobre el conjunto de test y vemos métrica\n",
    "loss, accuracy = model_bert.evaluate(X_test_enc['input_ids'], y_test)\n",
    "print(f\"Accuracy DistilBERT model: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
