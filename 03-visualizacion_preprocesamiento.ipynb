{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización y preprocesamiento de los datos\n",
    "\n",
    "En este notebook utilizaremos el conjunto que hemos inspeccionado y adecuado para crear unas sencillas representaciones de los datos  y posteriormente aplicar tecnica de preprocesamiento realizar un sencillo modelo que nos ayude a analizar los sentimientos descritos en las diferentes reseñas. Así, el siguiente script está dividido en los siguientes bloques:\n",
    "\n",
    "- **BLOQUE A**: carga de datos inspeccionados.\n",
    "- **BLOQUE B**: visualización. \n",
    "- **BLOQUE C**: preprocesamiento de los textos.\n",
    "- **BLOQUE D**: partición del conjunto de datos en train y test.\n",
    "- **BLOQUE E**: balanceo de los datos.\n",
    "- **BLOQUE F**: vectorización de los conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE A: Carga de datos\n",
    "Antes de comenzar, cargaremos los datos que han sido adecuados en nuestra fase anterior de limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos ya adecuados\n",
    "df = pd.read_csv(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Que dimensiones tiene el conjunto de datos?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras observaciones del conjunto\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE B: visualización\n",
    "\n",
    "En este bloque utilizaremos las librerias [matplotlib](https://matplotlib.org/) y [seaborn](https://seaborn.pydata.org/) para crear unas sencillas representaciones de los datos a modo general y descriptivo, mientras que  nos ayudaremos de la librería [wordcloud](https://amueller.github.io/word_cloud/) para poder crear visualizaciones acerca de los textos que vamos a analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras para la variable target\n",
    "sns.???(x='target', palette='viridis', data=???)\n",
    "plt.???('Distribución de la variable target')\n",
    "plt.???('Frecuencia')\n",
    "plt.???('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 'pie' con porcentajes para la variable objetivo \n",
    "plt.???(???, autopct=\"%.2f%%\", labels=['HAM', 'SPAM'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de la distribución de las logitudes de los SMS.\n",
    "# Utilizamos los histogramas proporcionados por el propio dataframe.\n",
    "df[???].hist()\n",
    "plt.???('Histograma de Longitudes')\n",
    "plt.???('Longitud')\n",
    "plt.???('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de la longitud por cada tipo de clase - gráfico boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=???, y=???, data=df, palette='Set2')\n",
    "plt.title('Boxplot de Longitud por Polaridad')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Longitud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un gráfico de las palabras más comunes en los SMS para cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los textos por sus etiquetas\n",
    "\n",
    "# textos no spam\n",
    "ham_texts = df.???[df['target']==???, 'text']\n",
    "\n",
    "# textos spam\n",
    "spam_texts = df.???[df['target']==???, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para poder realizar el gráfico\n",
    "def wordcloud_draw(data, color, title):\n",
    "    words = ' '.join(data)\n",
    "    wordcloud = WordCloud(stopwords=stopwords.words('english'),\n",
    "                          background_color=color,\n",
    "                          width=2500,height=2000).generate(words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los dos gráficos en una sola visualización\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.subplot(1,2,1)\n",
    "wordcloud_draw(???,'white','Palabras más comunes en NO SPAM')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "wordcloud_draw(???, 'grey','Palabras más comunes en SPAM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE C: Preprocesamiento del texto\n",
    "\n",
    "El preprocesamiento del texto es una fase importante dentro del Procesamiento del Lenguaje Natural (NLP). El objetivo de esta fase es la de transformar el texto en crudo, de manera que sea más fácilmente consumible por los algoritmos y modelos de Machine Learning (ML) y Deep Learning (DL) a aplicar.\n",
    "\n",
    "Esta fase consta de diferentes pasos y no son siempre los mismos. En este caso, preprocesaremos los SMS de la siguiente manera:\n",
    "\n",
    "1. **Lower Casing**: Transformar mayúsculas a minúsculas.\n",
    "\n",
    "2. **Eliminar Non-Alphabets**: Reemplazar todos los caracteres que no aparecen en el abecedario por un espacio.\n",
    "\n",
    "3. **Eliminar letras consecutivas**: 3 o más letras consecutivas son reemplazadas por 2 letras (ejemplo: \"Heyyyy\" por \"Heyy\").\n",
    "\n",
    "4. **Tokenizacíon**:  proceso de dividir un texto en unidades más pequeñas llamadas tokens (en este caso, palabras).\n",
    "\n",
    "5. **Eliminar Stopwords**: Las Stopwords son aquellas palabras que no tienen un significado específico por si solas, por lo que pueden ser ignoradas sin sacrificar el significado de la oración (ejemplos en inglés: \"the\", \"a\").\n",
    "\n",
    "6. **Eliminar palabras cortas**: Palabras con menos de 2 letras son eliminadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar el texto en crudo\n",
    "def preprocess(text):    \n",
    "\n",
    "    # Definir patrones para reemplazar/eliminar.\n",
    "    alphaPattern      = \"[^a-zA-Z]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1\\1*\"\n",
    "    seqReplacePattern = r\"\\1\\1\"    \n",
    "    \n",
    "    # Crear lista de stopwords\n",
    "    en_stop =  set(stopwords.words('english')) - {'not','no'}  # o definirlo manualmente set(['a', 'an', 'the', 'in', 'does', 'do'])\n",
    "\n",
    "    # Lower Casing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Reemplazar non-alphabets.\n",
    "    text = re.sub(alphaPattern, \" \", text)\n",
    "\n",
    "    # Reemplazar letras consecutivas.\n",
    "    text = re.sub(sequencePattern, seqReplacePattern, text)\n",
    "    \n",
    "    # Tokenizar texto\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    # Eliminar tokens con menos de dos elementos/caracteres\n",
    "    tokens = [word for word in tokens if len(word)>2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función a cada una de los SMS\n",
    "df['preprocess_text'] = df[???].apply(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del preprocesamiento: un ejemplo\n",
    "print('SMS en crudo:', df.loc[0, ???])\n",
    "print('SMS preprocesado:', df.loc[0, ???])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE D: Partición del conjunto de datos en train y test (80,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['preprocess_text']\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(???, ???, test_size=???, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca de los conjuntos\n",
    "print('Tamaño del conjunto de entrenamiento:', ???(X_train))\n",
    "print('Tamaño del conjunto de test:', ???(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE D: Balanceo de los datos\n",
    "\n",
    "Si hablamos de problemas de modelado centrados en la clasificación, podemos encontrarnos con **conjuntos de datos** en las que la variable objetivo contiene clases muy **desequilibradas**, es decir, categorías con frecuencias muy diferentes.\n",
    "\n",
    "Al entrenar un modelo de clasificación con la variable no balanceada, encontraremos algunos problemas. Esto sucede porque el patrón de datos de la clase dominante superará a los de la clase con menos frecuencia. Como una de las clases tiene una frecuencia muy alta, el modelo construido con datos desequilibrados puede presentar una precisión muy alta y aun así no predecir correctamente ninguna observación para la clase con una frecuencia más baja. Esto puede dar la falsa impresión de que el modelo funciona bien cuando en realidad no es así.\n",
    "\n",
    "Para solucionar estos problemas podemos recurrir a una solución: **equilibrar o balancear los datos de la variable objetivo**.\n",
    "\n",
    "Como los modelos deben funcionar en el mundo real, este balanceo solo es necesario realizarlo en el **conjunto de entrenamiento**. De esta manera, el conjunto de test sobre el que evaluaremos el rendimiento de nuestros modelos seguirá reflejando lo más fielmente posible la realidad a la que se aplicarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar conjunto de datos de entrenamiento según clases\n",
    "X_train_spam = X_train[y_train == 'spam']\n",
    "y_train_spam = y_train[y_train == 'spam']\n",
    "\n",
    "X_train_no_spam = X_train[y_train == 'ham']\n",
    "y_train_no_spam = y_train[y_train == 'ham']\n",
    "\n",
    "# Submuestrear (undesampling) la clase mayoritaria\n",
    "X_train_no_spam_resampled, y_train_no_spam_resampled = resample(???, \n",
    "                                                                ???,\n",
    "                                                                replace=False,  # sin reemplazo\n",
    "                                                                n_samples=len(???),  # igualar tamaño al de \"spam\"\n",
    "                                                                random_state=42)\n",
    "\n",
    "# Combinar clases balanceadas\n",
    "X_train_balanced = pd.concat([X_train_spam, X_train_no_spam_resampled])\n",
    "y_train_balanced = pd.concat([y_train_spam, y_train_no_spam_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca de los conjuntos tras balanceo\n",
    "print('Tamaño del conjunto de entrenamiento:', len(X_train_balanced))\n",
    "print('Tamaño del conjunto de test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias relativas de la variable 'target' en el conjunto de entrenamiento\n",
    "round(y_train_balanced.value_counts(normalize=True), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias relativas de 'target' en el conjunto de test\n",
    "round(y_test.value_counts(normalize=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE E: Vectorización del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de dar el texto en input a un modelo es necesario vectorizarlo: convertir las palabras en números.\n",
    "\n",
    "La conversión del texto en una representación númerica es uno de los pasos más importantes dentro de cualquier *pipeline* de NLP. Esta conversión resulta esencial para que las \"máquinas\" puedan comprender y decodificar patrones dentro de cualquier lenguaje.\n",
    "\n",
    "Se trata de un proceso iterativo y que puede ser realizado mediante múltiples maneras o técnicas, abarcando desde las representaciones más sencillas (por ejemlo, One hot encoding) hasta otras más \"inteligentes\", que logran tener en cuenta las similitudes y diferencias entre ellas al basar su aprendizaje en redes neuronales (Word embeddings).\n",
    "\n",
    "En este caso vamos a utilizar la técnica **TF-IDF (Term Frequency-Inverse Document Frequency)**. A continuación, se describen los conceptos clave:\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "Mide la frecuencia de un término específico en un texto.\n",
    "Se calcula dividiendo el número de veces que un término aparece en un texto entre el número total de términos en el propio texto.\n",
    "Cuanto más frecuente es un término, mayor es su valor de TF.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "Mide la importancia de un término en el conjunto de textos.\n",
    "Se calcula tomando el logaritmo del inverso de la proporción de textos que contienen el término.\n",
    "Así, los términos que aparecen en muchos textos tendrán un IDF más bajo, ya que se consideran menos informativos.\n",
    "\n",
    "3. **TF-IDF**:\n",
    "Combina TF y IDF para asignar un peso a cada término en cada texto: **TF-IDF = TF * IDF** .\\\n",
    "Los términos que son frecuentes en un texto pero raros en el conjunto de textos tendrán un alto valor de TF-IDF, lo que indica su importancia relativa en ese texto específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización de textos\n",
    "# Cargamos el vectorizador\n",
    "vectorizer = TfidfVectorizer() \n",
    "\n",
    "# fit_transform() determina qué palabras existen en el conjunto de datos y asigna un índice a cada una de ellas\n",
    "X_train_vec = vectorizer.???([\" \".join(tokens) for tokens in ???])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos nuevos datos (conjunto de test) en función del vocabulario aprendido anteriormente (con el conjunto de entrenamiento)\n",
    "X_test_vec = vectorizer.???([\" \".join(tokens) for tokens in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver con más detalle el objecto generado con `TfidfVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se trata de una matriz dispersa (*sparse matrix*) en formato CSR (*Compressed Sparse Row*). Una matriz dispersa es una estructura de datos que se utiliza para almacenar matrices que tienen una gran cantidad de elementos cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener dimensiones\n",
    "num_textos, num_terminos = X_train_vec.???\n",
    "\n",
    "print(f\"Número de Textos: {num_textos}\")\n",
    "print(f\"Número de Términos: {num_terminos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos 10 palabras al azar  \n",
    "random.???(list(vectorizer.get_feature_names_out()), ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos el valor TF-IDF asignado a algunas palabras en el primer texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamos los términos obtenidos mediante el objeto vectorizer para utilizarlos de manera más directa\n",
    "terminos = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Primer documento como vector TF-IDF\n",
    "vector_tfidf_primer_documento = X_train_vec[0]\n",
    "\n",
    "# Crear un DataFrame para visualizar el resultado\n",
    "df_terminos = pd.DataFrame(vector_tfidf_primer_documento.toarray(), columns=terminos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor TF-IDF asignado a una palabra contenida en el texto\n",
    "df_terminos[???]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor TF-IDF asignado a una palabra no presente en el texto\n",
    "df_terminos[???]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE E: Exportación de los conjuntos preprocesados\n",
    "\n",
    "En esta última fase exportamos y guardamos los conjuntos preprocesados en la carpeta 'data', para poderlos utilizar en las próximas tareas de modelado. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿De que tipo son los conjuntos resultantes?\n",
    "print(f'X_train: {???(X_train_balanced)}')\n",
    "print(f'X_train_vec: {???(X_train_vec)}')\n",
    "print(f'y_train: {???(y_train_balanced)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a guardarlos como archivos binarios utilizando **pickle**, que es una forma general de serializar objetos en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los conjuntos X\n",
    "with open('../data/x_train.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_balanced, file)\n",
    "\n",
    "with open('../data/x_test.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los conjuntos de textos vectorizados\n",
    "with open('../data/x_train_vec.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_vec, file)\n",
    "\n",
    "with open('../data/x_test_vec.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test_vec, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos las variables objetivo\n",
    "with open('../data/y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train_balanced, file)\n",
    "\n",
    "with open('../data/y_test.pkl', 'wb') as file:\n",
    "    pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el objeto vectorizer en un fichero \n",
    "joblib.dump(vectorizer, '../models/tfidf_vectorizer.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
